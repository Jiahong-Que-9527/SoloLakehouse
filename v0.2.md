下面给你一套 **v0.2（最小补齐清单）** 的**可运行代码包**，严格围绕你选定的架构：

> **Delta table + Spark（Colab 或本地） + MLflow + MinIO**
> 补齐：**Catalog/Metadata、Quality Gate、Lineage、Orchestration（Makefile）、Batch Inference**

我按你之前的 repo 结构来放文件。你可以直接复制创建。

---

# 目录与新增文件

```
sololakehouse/
├─ tables.yaml
├─ Makefile
├─ infra/
│  ├─ docker-compose.yml
│  └─ env.example
├─ pipelines/
│  ├─ bootstrap/
│  │  ├─ spark_session.py
│  │  └─ bootstrap.py
│  ├─ quality/
│  │  └─ checks.py
│  ├─ lineage/
│  │  └─ log_lineage.py
│  └─ inference/
│     └─ batch_score.py
└─ data/
   └─ sample/
      └─ data.csv
```

---

# 1) `tables.yaml`（Catalog / 表目录）

```yaml
project:
  name: sololakehouse
  env: dev

storage:
  s3_bucket: ${S3_BUCKET}
  s3_prefix: ${S3_PREFIX}   # e.g. sololakehouse/dev
  base_uri: s3a://${S3_BUCKET}/${S3_PREFIX}

tables:
  bronze_raw:
    description: "Bronze raw ingestion table (Delta)."
    path: ${base_uri}/tables/bronze_raw
    primary_keys: ["id"]
    timestamp_col: null

  features_v1:
    description: "Offline feature store table (Delta)."
    path: ${base_uri}/tables/features_v1
    primary_keys: ["id"]
    timestamp_col: "feature_ts"

  training_set_v1:
    description: "Gold training dataset generated from PIT join (Delta)."
    path: ${base_uri}/tables/training_set_v1
    primary_keys: ["id"]
    timestamp_col: "label_ts"

  predictions_v1:
    description: "Batch predictions output table (Delta)."
    path: ${base_uri}/tables/predictions_v1
    primary_keys: ["id"]
    timestamp_col: "scored_ts"

  lineage_runs:
    description: "Lineage log table (Delta): data version + code version + mlflow run."
    path: ${base_uri}/tables/__lineage_runs
    primary_keys: ["run_id"]
    timestamp_col: "event_ts"
```

---

# 2) `Makefile`（一键跑闭环）

```makefile
SHELL := /bin/bash

.PHONY: help bootstrap bronze features train batch-score quality lineage

help:
	@echo "SoloLakehouse v0.2 commands:"
	@echo "  make bootstrap     - Start infra (MinIO + MLflow + Postgres) and init buckets"
	@echo "  make bronze        - Load sample CSV -> Bronze Delta table"
	@echo "  make features      - Build feature store Delta table"
	@echo "  make batch-score   - Batch inference -> Predictions Delta table"
	@echo ""
	@echo "Tips:"
	@echo "  - Copy infra/env.example -> infra/.env and edit keys first."
	@echo "  - Run: docker compose -f infra/docker-compose.yml --env-file infra/.env up -d"

bootstrap:
	python -m pipelines.bootstrap.bootstrap

bronze:
	python -c "from pipelines.bootstrap.spark_session import get_spark; from pipelines.quality.checks import run_quality_gate; import pandas as pd; \
import os; \
spark=get_spark(); \
df=spark.read.option('header','true').csv('data/sample/data.csv', inferSchema=True); \
from pipelines.bootstrap.spark_session import load_catalog, table_path; cat=load_catalog('tables.yaml'); \
path=table_path(cat,'bronze_raw'); \
df.write.format('delta').mode('overwrite').save(path); \
run_quality_gate(spark, cat, 'bronze_raw'); \
from pipelines.lineage.log_lineage import log_lineage; log_lineage(spark, cat, table_name='bronze_raw', action='write_overwrite'); \
print('Bronze done ->', path)"

features:
	python -c "from pipelines.bootstrap.spark_session import get_spark, load_catalog, table_path; from pipelines.quality.checks import run_quality_gate; \
from pipelines.lineage.log_lineage import log_lineage; \
import pyspark.sql.functions as F; \
spark=get_spark(); cat=load_catalog('tables.yaml'); \
bronze_path=table_path(cat,'bronze_raw'); feat_path=table_path(cat,'features_v1'); \
bronze=spark.read.format('delta').load(bronze_path); \
features=(bronze \
  .withColumn('feature_ts', F.current_timestamp()) \
  .withColumn('x1_scaled', F.col('x1')*F.lit(1.0)) \
  .withColumn('x2_scaled', F.col('x2')*F.lit(1.0)) \
  .select('id','feature_ts','x1','x2','x1_scaled','x2_scaled','label')); \
features.write.format('delta').mode('overwrite').save(feat_path); \
run_quality_gate(spark, cat, 'features_v1'); \
log_lineage(spark, cat, table_name='features_v1', action='write_overwrite'); \
print('Features done ->', feat_path)"

batch-score:
	python -m pipelines.inference.batch_score
```

> 说明：为了让你“先跑通”，`bronze/features` 这里用的是极简 `python -c ...`。
> 之后你要“平台化”，我建议你把这两段拆成 `pipelines/bronze/run.py`、`pipelines/features/build.py`（我也可以下一步帮你拆干净）。

---

# 3) `infra/docker-compose.yml`（MinIO + Postgres + MLflow）

```yaml
version: "3.8"

services:
  postgres:
    image: postgres:16
    container_name: sololakehouse-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "5432:5432"
    volumes:
      - ./volumes/postgres:/var/lib/postgresql/data

  minio:
    image: minio/minio:RELEASE.2024-12-18T13-15-44Z
    container_name: sololakehouse-minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./volumes/minio:/data

  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.14.3
    container_name: sololakehouse-mlflow
    depends_on:
      - postgres
      - minio
    environment:
      MLFLOW_S3_ENDPOINT_URL: ${MLFLOW_S3_ENDPOINT_URL}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
    ports:
      - "5000:5000"
    command: >
      mlflow server
      --host 0.0.0.0
      --port 5000
      --backend-store-uri postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      --default-artifact-root s3://${MLFLOW_ARTIFACT_BUCKET}/mlflow
```

---

# 4) `infra/env.example`（统一 secrets & endpoints）

```env
# ---------- MinIO ----------
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin123

# Host access (your local machine / Colab)
MINIO_ENDPOINT=http://localhost:9000
MINIO_CONSOLE=http://localhost:9001

# S3 creds (used by Spark + MLflow)
AWS_ACCESS_KEY_ID=minioadmin
AWS_SECRET_ACCESS_KEY=minioadmin123

# ---------- MLflow ----------
MLFLOW_TRACKING_URI=http://localhost:5000
MLFLOW_S3_ENDPOINT_URL=http://minio:9000   # inside docker network for MLflow container
MLFLOW_ARTIFACT_BUCKET=mlflow-artifacts

# ---------- Postgres (MLflow backend) ----------
POSTGRES_USER=mlflow
POSTGRES_PASSWORD=mlflow123
POSTGRES_DB=mlflow

# ---------- SoloLakehouse data lake ----------
S3_BUCKET=sololakehouse
S3_PREFIX=dev
```

> 注意：
>
> * `MLFLOW_S3_ENDPOINT_URL` 这里写 `http://minio:9000` 是因为它在 docker 网络内。
> * 你在本机/Colab 访问 MinIO，要用 `MINIO_ENDPOINT=http://localhost:9000`（或你的公网域名）。

---

# 5) `pipelines/bootstrap/spark_session.py`（Spark + Delta + S3A 配置）

```python
import os
import yaml
from typing import Any, Dict

from pyspark.sql import SparkSession


def get_spark(app_name: str = "sololakehouse") -> SparkSession:
    """
    Spark session configured for:
    - Delta Lake
    - S3A (MinIO)
    Works in Colab and local environments if Spark+Delta are installed.
    """
    minio_endpoint = os.environ.get("MINIO_ENDPOINT", "http://localhost:9000")
    access_key = os.environ.get("AWS_ACCESS_KEY_ID", "")
    secret_key = os.environ.get("AWS_SECRET_ACCESS_KEY", "")

    builder = (
        SparkSession.builder.appName(app_name)
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
        # S3A / MinIO
        .config("spark.hadoop.fs.s3a.endpoint", minio_endpoint)
        .config("spark.hadoop.fs.s3a.access.key", access_key)
        .config("spark.hadoop.fs.s3a.secret.key", secret_key)
        .config("spark.hadoop.fs.s3a.path.style.access", "true")
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        # Avoid some metadata pitfalls
        .config("spark.sql.shuffle.partitions", "8")
    )

    spark = builder.getOrCreate()
    spark.sparkContext.setLogLevel("WARN")
    return spark


def _expand_env_vars(s: str) -> str:
    # Expand ${VAR} placeholders from environment.
    return os.path.expandvars(s)


def load_catalog(path: str = "tables.yaml") -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        raw = f.read()
    raw = _expand_env_vars(raw)
    return yaml.safe_load(raw)


def table_path(catalog: Dict[str, Any], table_name: str) -> str:
    tables = catalog.get("tables", {})
    if table_name not in tables:
        raise KeyError(f"Table not found in catalog: {table_name}")
    return tables[table_name]["path"]
```

---

# 6) `pipelines/bootstrap/bootstrap.py`（初始化 buckets）

```python
import os
import boto3
from botocore.client import Config


def ensure_bucket(s3, name: str) -> None:
    buckets = [b["Name"] for b in s3.list_buckets().get("Buckets", [])]
    if name in buckets:
        print(f"[OK] bucket exists: {name}")
        return
    s3.create_bucket(Bucket=name)
    print(f"[CREATE] bucket: {name}")


def main():
    endpoint = os.environ.get("MINIO_ENDPOINT", "http://localhost:9000")
    access_key = os.environ.get("AWS_ACCESS_KEY_ID", "")
    secret_key = os.environ.get("AWS_SECRET_ACCESS_KEY", "")

    lake_bucket = os.environ.get("S3_BUCKET", "sololakehouse")
    artifact_bucket = os.environ.get("MLFLOW_ARTIFACT_BUCKET", "mlflow-artifacts")

    s3 = boto3.client(
        "s3",
        endpoint_url=endpoint,
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key,
        config=Config(signature_version="s3v4"),
        region_name="us-east-1",
    )

    ensure_bucket(s3, lake_bucket)
    ensure_bucket(s3, artifact_bucket)

    print("[DONE] bootstrap complete.")


if __name__ == "__main__":
    main()
```

---

# 7) `pipelines/quality/checks.py`（Quality Gate：最小但实用）

```python
from __future__ import annotations
from typing import Dict, Any, List, Tuple

import pyspark.sql.functions as F
from pyspark.sql import SparkSession, DataFrame

from pipelines.bootstrap.spark_session import table_path


def _basic_stats(df: DataFrame, cols: List[str]) -> DataFrame:
    exprs = []
    for c in cols:
        exprs += [
            F.count(F.col(c)).alias(f"{c}__count"),
            F.countDistinct(F.col(c)).alias(f"{c}__nunique"),
            F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(f"{c}__nulls"),
        ]
    return df.agg(*exprs)


def run_quality_gate(spark: SparkSession, catalog: Dict[str, Any], table_name: str) -> None:
    """
    Minimal quality gate:
    - table must be readable
    - must have rows
    - key columns must not be null (if declared)
    - timestamp col must not be null (if declared)
    """
    tdef = catalog["tables"][table_name]
    path = table_path(catalog, table_name)

    df = spark.read.format("delta").load(path)

    n = df.count()
    if n == 0:
        raise ValueError(f"[QUALITY] {table_name} has 0 rows: {path}")

    primary_keys = tdef.get("primary_keys") or []
    ts_col = tdef.get("timestamp_col")

    required_cols = list(primary_keys)
    if ts_col:
        required_cols.append(ts_col)

    for c in required_cols:
        if c not in df.columns:
            raise ValueError(f"[QUALITY] {table_name} missing required col: {c}")

        nulls = df.filter(F.col(c).isNull()).count()
        if nulls > 0:
            raise ValueError(f"[QUALITY] {table_name} column {c} has nulls: {nulls}")

    # Print small report (can be logged to MLflow later)
    cols_preview = [c for c in df.columns[:6]]
    stats = _basic_stats(df, cols_preview).collect()[0].asDict()
    print(f"[QUALITY] {table_name}: rows={n}, preview_cols={cols_preview}")
    print(f"[QUALITY] preview stats: {stats}")
```

---

# 8) `pipelines/lineage/log_lineage.py`（Lineage：delta version + git + mlflow run）

```python
from __future__ import annotations
import os
import subprocess
from datetime import datetime, timezone
from typing import Dict, Any, Optional

import mlflow
import pyspark.sql.functions as F
from pyspark.sql import SparkSession

from pipelines.bootstrap.spark_session import table_path


def _git_commit() -> str:
    try:
        return (
            subprocess.check_output(["git", "rev-parse", "HEAD"], stderr=subprocess.DEVNULL)
            .decode("utf-8")
            .strip()
        )
    except Exception:
        return "UNKNOWN"


def _delta_version(spark: SparkSession, delta_path: str) -> Optional[int]:
    try:
        hist = spark.sql(f"DESCRIBE HISTORY delta.`{delta_path}`")
        row = hist.select(F.max(F.col("version")).alias("v")).collect()[0]
        return int(row["v"])
    except Exception:
        return None


def log_lineage(
    spark: SparkSession,
    catalog: Dict[str, Any],
    table_name: str,
    action: str,
    extra: Optional[Dict[str, str]] = None,
) -> None:
    """
    Writes a lineage record into a Delta table:
    __lineage_runs (event_ts, table_name, table_path, delta_version, action, git_commit, mlflow_run_id)
    Also logs to MLflow tags if a run is active.
    """
    lineage_path = table_path(catalog, "lineage_runs")
    target_path = table_path(catalog, table_name)

    dv = _delta_version(spark, target_path)
    commit = _git_commit()
    event_ts = datetime.now(timezone.utc).isoformat()

    active = mlflow.active_run()
    run_id = active.info.run_id if active else None

    rows = [{
        "event_ts": event_ts,
        "table_name": table_name,
        "table_path": target_path,
        "delta_version": dv,
        "action": action,
        "git_commit": commit,
        "run_id": run_id,
    }]

    if extra:
        # store extra as a compact string map: k=v;...
        rows[0]["extra"] = ";".join([f"{k}={v}" for k, v in extra.items()])
    else:
        rows[0]["extra"] = ""

    df = spark.createDataFrame(rows)
    df.write.format("delta").mode("append").save(lineage_path)

    print(f"[LINEAGE] wrote -> {lineage_path}: {rows[0]}")

    if active:
        mlflow.set_tag("git_commit", commit)
        mlflow.set_tag(f"delta.{table_name}.path", target_path)
        if dv is not None:
            mlflow.set_tag(f"delta.{table_name}.version", str(dv))
        mlflow.set_tag("lineage_logged", "true")
```

---

# 9) `pipelines/inference/batch_score.py`（Batch Inference：读 features → MLflow 模型 → 写 predictions Delta）

```python
from __future__ import annotations
import os
from datetime import datetime, timezone

import mlflow
import mlflow.pyfunc
import pyspark.sql.functions as F

from pipelines.bootstrap.spark_session import get_spark, load_catalog, table_path
from pipelines.quality.checks import run_quality_gate
from pipelines.lineage.log_lineage import log_lineage


def resolve_model_uri() -> str:
    """
    Priority:
    1) MODEL_URI env (explicit)
    2) MODEL_NAME + MODEL_STAGE env (default: Staging)
    """
    model_uri = os.environ.get("MODEL_URI")
    if model_uri:
        return model_uri

    model_name = os.environ.get("MODEL_NAME", "sololakehouse-demo-model")
    stage = os.environ.get("MODEL_STAGE", "Staging")
    return f"models:/{model_name}/{stage}"


def main():
    # MLflow tracking
    tracking_uri = os.environ.get("MLFLOW_TRACKING_URI", "http://localhost:5000")
    mlflow.set_tracking_uri(tracking_uri)

    spark = get_spark("sololakehouse-batch-score")
    catalog = load_catalog("tables.yaml")

    feat_path = table_path(catalog, "features_v1")
    pred_path = table_path(catalog, "predictions_v1")

    features = spark.read.format("delta").load(feat_path)

    # Convert to pandas for MLflow pyfunc (simple & robust for v0.2 sample)
    # For big data, you’d implement pandas_udf or Spark model scoring.
    pdf = features.select("id", "x1", "x2").toPandas()

    model_uri = resolve_model_uri()
    print("[MODEL] loading:", model_uri)
    model = mlflow.pyfunc.load_model(model_uri)

    yhat = model.predict(pdf[["x1", "x2"]])
    pdf["prediction"] = yhat

    scored_ts = datetime.now(timezone.utc).isoformat()
    out_df = spark.createDataFrame(pdf).withColumn("scored_ts", F.to_timestamp(F.lit(scored_ts)))

    out_df.write.format("delta").mode("overwrite").save(pred_path)

    run_quality_gate(spark, catalog, "predictions_v1")
    log_lineage(spark, catalog, table_name="predictions_v1", action="write_overwrite", extra={"model_uri": model_uri})

    print("[DONE] batch predictions written ->", pred_path)


if __name__ == "__main__":
    main()
```

---

# 10) `data/sample/data.csv`（你已有也可复用）

```csv
id,x1,x2,label
1,0.1,1.2,0
2,0.2,1.1,0
3,1.2,0.1,1
4,1.1,0.2,1
```

---

## 你现在拥有的“平台感”能力（对应缺口清单）

✅ Catalog：`tables.yaml`
✅ Quality Gate：`pipelines/quality/checks.py`
✅ Lineage：`pipelines/lineage/log_lineage.py`（Delta version + git commit + mlflow run tag）
✅ Orchestration：`Makefile`
✅ Batch Inference：`pipelines/inference/batch_score.py`

---

# 如何跑（本地）

1. 启动基础设施

```bash
cp infra/env.example infra/.env
docker compose -f infra/docker-compose.yml --env-file infra/.env up -d
```

2. 初始化 buckets

```bash
export $(cat infra/.env | xargs)
make bootstrap
```

3. 跑 Bronze / Features

```bash
make bronze
make features
```

4. （前提：你已经用 MLflow 注册了一个模型到 `sololakehouse-demo-model` 的 `Staging`）
   然后跑批量推理：

```bash
export MODEL_NAME=sololakehouse-demo-model
export MODEL_STAGE=Staging
make batch-score
```

---

# 针对 Colab（Spark on Colab）你还需要做的一步

Colab 里你要确保安装 Spark + Delta（你之前应该已经在做）。然后：

* 设环境变量（MINIO_ENDPOINT/KEY/SECRET/MLFLOW_TRACKING_URI）
* 运行同样的 python 脚本即可

如果你希望我把 **Colab 一键安装与初始化 Notebook（包括安装 Spark/Delta、连 MinIO、写 Delta、连 MLflow）** 也给你，我可以直接给你一个 `notebooks/00_colab_setup.ipynb` 的脚本版内容。

---

## 下一步我建议你做什么（最小升级，立刻更像 Databricks）

1. 把 `bronze/features` 从 Makefile 的 `python -c` 拆成独立脚本（结构化日志 + MLflow run）
2. 增加一个 `training/train.py`（训练时自动：

   * 读取 features
   * 记录 MLflow
   * 注册模型到 `MODEL_NAME`
   * 同时写 lineage）

你回我一句：
**“继续，把 v0.2 的 training/train.py（含注册模型）也补齐，并把 bronze/features 拆脚本化”**
我就把剩下的“闭环最后一块”一次性给你。
